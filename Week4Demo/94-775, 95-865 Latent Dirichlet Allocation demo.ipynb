{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 94-775/95-865: Topic Modeling with Latent Dirichlet Allocation\n",
    "\n",
    "Author: George H. Chen (georgechen [at symbol] cmu.edu)\n",
    "\n",
    "The beginning part of this demo is a shortened and modified version of sklearn's LDA & NMF demo (http://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=5, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load in 10,000 posts from the 20 Newsgroups dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "num_articles = 10000\n",
    "data = fetch_20newsgroups(shuffle=True, random_state=0,\n",
    "                          remove=('headers', 'footers', 'quotes')).data[:num_articles]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that there are 10,000 posts, and we can look at an example post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Experimental Lyme Disease in Dogs Produces Arthritis and Persistant Infection,\n",
      "The Journal of Infectious Diseases, March 1993, 167:651-664\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# you can take a look at what individual documents look like by replacing what index we look at\n",
    "print(data[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now fit a `CountVectorizer` model that will compute, for each post, what its raw word count histograms are (the \"term frequencies\" we saw in week 1).\n",
    "\n",
    "The output of the following cell is the term-frequencies matrix, where rows index different posts/text documents, and columns index 1000 different vocabulary words. A note about the arguments to `CountVectorizer`:\n",
    "\n",
    "- `max_df`: we only keep words that appear in at most this fraction of the documents\n",
    "- `min_df`: we only keep words that appear in at least this many documents\n",
    "- `stop_words`: whether to remove stop words\n",
    "- `max_features`: among words that don't get removed due to the above 3 arguments, we keep the top `max_features` number of most frequently occuring words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# CountVectorizer does tokenization and can remove terms that occur too frequently, not frequently enough, or that are stop words\n",
    "\n",
    "# document frequency (df) means number of documents a word appears in\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95,\n",
    "                                min_df=2,\n",
    "                                stop_words='english',\n",
    "                                max_features=vocab_size)\n",
    "tf = tf_vectorizer.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that there are 10,000 rows (corresponding to posts), and 1000 columns (corresponding to words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A note about the `tf` matrix: this actually is stored as what's called a sparse matrix (rather than a 2D NumPy array that you're more familiar with). The reason is that often these matrices are really large and the vast majority of entries are 0, so it's possible to save space by not storing where the 0's are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tf)# to save space, matrix only store none 0 values in it. most value in matrix is 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " To convert `tf` to a 2D NumPy table, you can run `tf.toarray()` (this does not modify the original `tf` variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.toarray().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can figure out what words the different columns correspond to by using the `get_feature_names()` function; the output is in the same order as the column indices. In particular, we can index into the following list (i.e., so given a column index, we can figure out which word it corresponds to)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '02', '03', '04', '0d', '0t', '10', '100', '11', '12', '128', '13', '14', '145', '15', '16', '17', '18', '19', '1990', '1991', '1992', '1993', '1d9', '1st', '1t', '20', '200', '21', '22', '23', '24', '25', '250', '26', '27', '28', '29', '2di', '2tm', '30', '300', '31', '32', '33', '34', '34u', '35', '36', '37', '38', '39', '3d', '3t', '40', '41', '42', '43', '44', '45', '46', '48', '50', '500', '55', '60', '64', '6um', '70', '75', '75u', '7ey', '80', '800', '86', '90', '91', '92', '93', '9v', 'a86', 'ability', 'able', 'ac', 'accept', 'access', 'according', 'act', 'action', 'actually', 'add', 'addition', 'address', 'administration', 'advance', 'age', 'ago', 'agree', 'ah', 'air', 'al', 'algorithm', 'allow', 'allowed', 'alt', 'america', 'american', 'analysis', 'anonymous', 'answer', 'answers', 'anti', 'anybody', 'apparently', 'appears', 'apple', 'application', 'applications', 'appreciate', 'appreciated', 'approach', 'appropriate', 'apr', 'april', 'archive', 'area', 'areas', 'aren', 'argument', 'armenia', 'armenian', 'armenians', 'arms', 'army', 'article', 'articles', 'ask', 'asked', 'asking', 'assume', 'atheism', 'attack', 'attempt', 'au', 'author', 'authority', 'available', 'average', 'avoid', 'away', 'ax', 'b8f', 'bad', 'base', 'based', 'basic', 'basically', 'basis', 'belief', 'believe', 'best', 'better', 'bh', 'bhj', 'bible', 'big', 'bike', 'bit', 'bits', 'bj', 'black', 'block', 'blood', 'board', 'body', 'book', 'books', 'bought', 'box', 'break', 'bring', 'brought', 'btw', 'buf', 'build', 'building', 'built', 'bus', 'business', 'buy', 'bxn', 'ca', 'cable', 'california', 'called', 'calls', 'came', 'canada', 'car', 'card', 'cards', 'care', 'carry', 'cars', 'case', 'cases', 'cause', 'cd', 'center', 'certain', 'certainly', 'chance', 'change', 'changed', 'changes', 'check', 'chicago', 'child', 'children', 'chip', 'chips', 'choice', 'christ', 'christian', 'christianity', 'christians', 'church', 'citizens', 'city', 'claim', 'claims', 'class', 'clear', 'clearly', 'clinton', 'clipper', 'close', 'code', 'color', 'com', 'come', 'comes', 'coming', 'command', 'comments', 'commercial', 'committee', 'common', 'community', 'comp', 'company', 'complete', 'completely', 'computer', 'condition', 'conference', 'congress', 'consider', 'considered', 'contact', 'contains', 'context', 'continue', 'control', 'controller', 'copy', 'correct', 'cost', 'couldn', 'country', 'couple', 'course', 'court', 'cover', 'create', 'created', 'crime', 'cross', 'cs', 'current', 'currently', 'cut', 'cx', 'data', 'date', 'dave', 'david', 'day', 'days', 'db', 'dc', 'dead', 'deal', 'death', 'dec', 'decided', 'defense', 'define', 'deleted', 'department', 'des', 'design', 'designed', 'details', 'development', 'device', 'devices', 'did', 'didn', 'difference', 'different', 'difficult', 'digital', 'directly', 'directory', 'discussion', 'disk', 'display', 'distribution', 'division', 'dod', 'does', 'doesn', 'doing', 'don', 'door', 'dos', 'doubt', 'dr', 'drive', 'driver', 'drivers', 'drives', 'drug', 'early', 'earth', 'easily', 'east', 'easy', 'ed', 'edu', 'effect', 'electronic', 'email', 'encryption', 'end', 'enforcement', 'engine', 'entire', 'entry', 'environment', 'error', 'escrow', 'especially', 'event', 'events', 'evidence', 'exactly', 'example', 'excellent', 'exist', 'existence', 'exists', 'expect', 'experience', 'explain', 'export', 'extra', 'face', 'fact', 'faith', 'false', 'family', 'faq', 'far', 'fast', 'faster', 'father', 'fax', 'fbi', 'features', 'federal', 'feel', 'field', 'figure', 'file', 'files', 'final', 'finally', 'fine', 'firearms', 'floppy', 'folks', 'follow', 'following', 'food', 'force', 'form', 'format', 'free', 'freedom', 'friend', 'ftp', 'function', 'functions', 'future', 'g9v', 'game', 'games', 'gas', 'gave', 'general', 'generally', 'gets', 'getting', 'gif', 'given', 'gives', 'giz', 'gk', 'gm', 'goal', 'god', 'goes', 'going', 'good', 'got', 'gov', 'government', 'graphics', 'great', 'greek', 'ground', 'group', 'groups', 'guess', 'gun', 'guns', 'guy', 'half', 'hand', 'happen', 'happened', 'happens', 'hard', 'hardware', 'haven', 'having', 'head', 'health', 'hear', 'heard', 'held', 'hell', 'help', 'hi', 'high', 'higher', 'history', 'hit', 'hockey', 'hold', 'home', 'hope', 'hours', 'house', 'hp', 'human', 'ibm', 'ide', 'idea', 'ideas', 'ii', 'image', 'images', 'imagine', 'important', 'include', 'included', 'includes', 'including', 'individual', 'info', 'information', 'input', 'inside', 'installed', 'instead', 'insurance', 'int', 'interested', 'interesting', 'interface', 'internal', 'international', 'internet', 'involved', 'isn', 'israel', 'israeli', 'issue', 'issues', 'jesus', 'jewish', 'jews', 'jim', 'job', 'jobs', 'john', 'jpeg', 'just', 'key', 'keyboard', 'keys', 'kill', 'killed', 'kind', 'knew', 'know', 'knowledge', 'known', 'knows', 'la', 'land', 'language', 'large', 'late', 'later', 'law', 'laws', 'league', 'learn', 'leave', 'left', 'legal', 'let', 'letter', 'level', 'library', 'life', 'light', 'like', 'likely', 'limited', 'line', 'lines', 'list', 'little', 'live', 'lives', 'living', 'll', 'local', 'long', 'longer', 'look', 'looked', 'looking', 'looks', 'lord', 'lost', 'lot', 'lots', 'love', 'low', 'lower', 'mac', 'machine', 'machines', 'mail', 'main', 'major', 'make', 'makes', 'making', 'man', 'manager', 'manual', 'mark', 'market', 'mass', 'master', 'material', 'matter', 'max', 'maybe', 'mb', 'mean', 'meaning', 'means', 'media', 'medical', 'members', 'memory', 'men', 'mention', 'mentioned', 'message', 'mike', 'miles', 'military', 'million', 'mind', 'mit', 'mode', 'model', 'modem', 'money', 'monitor', 'month', 'months', 'moral', 'mother', 'motif', 'mouse', 'mr', 'ms', 'multiple', 'nasa', 'national', 'nature', 'near', 'necessary', 'need', 'needed', 'needs', 'net', 'network', 'new', 'news', 'newsgroup', 'nhl', 'nice', 'night', 'non', 'normal', 'note', 'nsa', 'number', 'numbers', 'object', 'obvious', 'obviously', 'offer', 'office', 'official', 'oh', 'ok', 'old', 'ones', 'open', 'opinion', 'opinions', 'orbit', 'order', 'org', 'organization', 'original', 'os', 'output', 'outside', 'package', 'page', 'paper', 'particular', 'parts', 'party', 'past', 'paul', 'pay', 'pc', 'peace', 'people', 'perfect', 'performance', 'period', 'person', 'personal', 'phone', 'pick', 'picture', 'pin', 'pittsburgh', 'pl', 'place', 'places', 'plan', 'play', 'played', 'player', 'players', 'plus', 'point', 'points', 'police', 'policy', 'political', 'population', 'port', 'position', 'possible', 'possibly', 'post', 'posted', 'posting', 'power', 'pp', 'present', 'president', 'press', 'pretty', 'previous', 'price', 'printer', 'privacy', 'private', 'pro', 'probably', 'problem', 'problems', 'process', 'product', 'program', 'programs', 'project', 'protect', 'provide', 'provides', 'pub', 'public', 'published', 'purpose', 'qq', 'quality', 'question', 'questions', 'quite', 'radio', 'ram', 'range', 'rate', 'read', 'reading', 'real', 'really', 'reason', 'reasonable', 'reasons', 'received', 'recent', 'recently', 'record', 'red', 'reference', 'regular', 'related', 'release', 'religion', 'religious', 'remember', 'reply', 'report', 'reports', 'request', 'require', 'required', 'requires', 'research', 'resources', 'response', 'rest', 'result', 'results', 'return', 'right', 'rights', 'road', 'rom', 'room', 'round', 'rules', 'run', 'running', 'runs', 'russian', 'safety', 'said', 'sale', 'san', 'save', 'saw', 'say', 'saying', 'says', 'school', 'sci', 'science', 'scientific', 'screen', 'scsi', 'search', 'season', 'second', 'secret', 'section', 'secure', 'security', 'seen', 'self', 'sell', 'send', 'sense', 'sent', 'serial', 'series', 'server', 'service', 'set', 'shall', 'shipping', 'short', 'shot', 'shuttle', 'similar', 'simple', 'simply', 'sin', 'single', 'site', 'sites', 'situation', 'size', 'small', 'society', 'software', 'solution', 'son', 'soon', 'sorry', 'sort', 'sound', 'sounds', 'source', 'sources', 'south', 'soviet', 'space', 'special', 'specific', 'speed', 'spirit', 'st', 'standard', 'start', 'started', 'state', 'statement', 'states', 'station', 'stephanopoulos', 'steve', 'stop', 'story', 'stream', 'street', 'strong', 'study', 'stuff', 'subject', 'suggest', 'sun', 'support', 'supports', 'supposed', 'sure', 'systems', 'taken', 'takes', 'taking', 'talk', 'talking', 'tape', 'tar', 'tax', 'team', 'teams', 'technical', 'technology', 'tell', 'term', 'terms', 'test', 'text', 'thank', 'thanks', 'theory', 'thing', 'things', 'think', 'thinking', 'thought', 'time', 'times', 'title', 'tm', 'today', 'told', 'took', 'tools', 'total', 'trade', 'transfer', 'tried', 'true', 'truth', 'try', 'trying', 'turkey', 'turkish', 'turn', 'tv', 'type', 'uk', 'understand', 'unfortunately', 'unit', 'united', 'university', 'unix', 'unless', 'usa', 'use', 'used', 'useful', 'usenet', 'user', 'users', 'uses', 'using', 'usually', 'value', 'values', 'van', 'various', 've', 'version', 'vga', 'video', 'view', 'voice', 'volume', 'vs', 'wait', 'want', 'wanted', 'wants', 'war', 'washington', 'wasn', 'watch', 'water', 'way', 'ways', 'weapons', 'week', 'weeks', 'went', 'white', 'wide', 'widget', 'willing', 'win', 'window', 'windows', 'wish', 'wm', 'women', 'won', 'word', 'words', 'work', 'worked', 'working', 'works', 'world', 'worth', 'wouldn', 'write', 'writing', 'written', 'wrong', 'wrote', 'x11', 'xt', 'year', 'years', 'yes', 'york', 'young']\n"
     ]
    }
   ],
   "source": [
    "print(tf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also go in reverse: given a word, we can figure out which column index it corresponds to. To do this, we use the `vocabulary_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_vectorizer.vocabulary_['book']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can figure out what the raw counts are for the 0-th post as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 2, 3, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf[0].toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now fit an LDA model to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                          evaluate_every=-1, learning_decay=0.7,\n",
       "                          learning_method='online', learning_offset=10.0,\n",
       "                          max_doc_update_iter=100, max_iter=10,\n",
       "                          mean_change_tol=0.001, n_components=10, n_jobs=None,\n",
       "                          perp_tol=0.1, random_state=0, topic_word_prior=None,\n",
       "                          total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_topics = 10\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, learning_method='online', random_state=0)\n",
    "lda.fit(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fitting procedure determines the every topic's distribution over words; this information is stored in the `components_` attribute. There's a catch: we actually have to normalize to get the probability distributions (without this normalization, instead what the model has are pseudocounts for how often different words appear per topic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1000)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 36546.2359 ,  42637.17064,  49198.91281, 114036.11143,\n",
       "        25195.09295,  38394.6771 ,  28563.10527,  47579.92792,\n",
       "        39951.26973,  63662.21236])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_word_distributions = np.array([row / row.sum() for row in lda.components_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that each topic's word distribution sums to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_word_distributions.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also print out what the probabilities for the different words are for a specific topic. This isn't very easy to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00349 0.      0.      0.      0.      0.      0.      0.00305 0.00631\n",
      " 0.      0.00113 0.      0.00012 0.00001 0.      0.00176 0.      0.\n",
      " 0.      0.      0.00197 0.00141 0.00318 0.00074 0.      0.00227 0.\n",
      " 0.0031  0.      0.      0.00002 0.      0.      0.00021 0.001   0.\n",
      " 0.      0.      0.      0.      0.      0.00285 0.00329 0.      0.\n",
      " 0.      0.      0.      0.0004  0.      0.      0.      0.      0.\n",
      " 0.      0.00166 0.      0.      0.      0.      0.00041 0.      0.\n",
      " 0.00617 0.00381 0.      0.00245 0.      0.      0.00103 0.00112 0.\n",
      " 0.      0.00041 0.00124 0.      0.00207 0.00131 0.00144 0.0005  0.\n",
      " 0.      0.      0.00059 0.      0.0002  0.      0.      0.      0.00084\n",
      " 0.0021  0.00107 0.      0.      0.00008 0.00024 0.00128 0.00211 0.\n",
      " 0.      0.      0.      0.      0.      0.00116 0.      0.0036  0.00287\n",
      " 0.      0.      0.      0.      0.      0.      0.00038 0.00243 0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.00093 0.\n",
      " 0.00089 0.00019 0.0003  0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.00015 0.      0.00354 0.      0.      0.      0.\n",
      " 0.      0.      0.      0.00001 0.00628 0.      0.00171 0.      0.\n",
      " 0.00267 0.00255 0.00093 0.00008 0.00037 0.      0.      0.00061 0.00941\n",
      " 0.00544 0.      0.      0.      0.00284 0.      0.00079 0.      0.\n",
      " 0.00299 0.      0.      0.00029 0.0002  0.00107 0.00019 0.00084 0.00125\n",
      " 0.00056 0.00065 0.      0.00036 0.      0.      0.      0.      0.\n",
      " 0.00084 0.00222 0.      0.00153 0.00076 0.0017  0.0004  0.      0.0021\n",
      " 0.00487 0.      0.      0.00071 0.00101 0.00032 0.      0.00084 0.00018\n",
      " 0.      0.      0.00238 0.00003 0.      0.00213 0.00042 0.00088 0.00023\n",
      " 0.00195 0.      0.      0.      0.      0.      0.00081 0.      0.\n",
      " 0.      0.      0.      0.      0.00468 0.      0.      0.      0.00028\n",
      " 0.      0.00118 0.      0.      0.      0.      0.      0.00199 0.0009\n",
      " 0.00139 0.      0.00112 0.      0.      0.      0.      0.      0.\n",
      " 0.00074 0.00009 0.      0.00519 0.00243 0.00007 0.00193 0.00078 0.00148\n",
      " 0.      0.      0.00084 0.00124 0.      0.00079 0.00001 0.00075 0.\n",
      " 0.00033 0.      0.00108 0.      0.00492 0.00008 0.      0.00001 0.00085\n",
      " 0.      0.00059 0.00101 0.00051 0.      0.      0.0011  0.00402 0.\n",
      " 0.00418 0.00151 0.      0.00098 0.00021 0.00239 0.      0.      0.\n",
      " 0.00398 0.      0.0001  0.00015 0.      0.00008 0.      0.      0.\n",
      " 0.      0.      0.00361 0.00238 0.00077 0.00022 0.00004 0.      0.\n",
      " 0.      0.      0.      0.      0.      0.00441 0.00464 0.0003  0.001\n",
      " 0.00039 0.00472 0.      0.      0.      0.00047 0.      0.      0.\n",
      " 0.      0.00001 0.0029  0.      0.      0.00136 0.      0.00174 0.0034\n",
      " 0.00061 0.      0.      0.      0.00446 0.      0.      0.00035 0.\n",
      " 0.      0.      0.      0.00019 0.00034 0.      0.      0.00017 0.\n",
      " 0.00505 0.      0.      0.      0.00159 0.00008 0.      0.      0.00172\n",
      " 0.00064 0.00004 0.      0.      0.      0.      0.00279 0.      0.\n",
      " 0.      0.      0.      0.      0.      0.00156 0.      0.00057 0.\n",
      " 0.      0.00348 0.      0.00034 0.00005 0.      0.00012 0.00018 0.00174\n",
      " 0.      0.00172 0.      0.      0.00243 0.      0.0017  0.      0.\n",
      " 0.      0.00167 0.      0.      0.01418 0.      0.      0.00023 0.\n",
      " 0.00209 0.      0.      0.0013  0.      0.      0.      0.      0.0061\n",
      " 0.      0.00201 0.00431 0.01683 0.00761 0.      0.      0.      0.00723\n",
      " 0.      0.      0.00013 0.      0.00087 0.01618 0.00154 0.0023  0.00086\n",
      " 0.00011 0.00021 0.      0.00021 0.00142 0.      0.00075 0.00013 0.00112\n",
      " 0.00022 0.00029 0.      0.      0.      0.00035 0.      0.00312 0.00122\n",
      " 0.      0.00547 0.00997 0.      0.00466 0.00157 0.      0.0016  0.\n",
      " 0.      0.      0.      0.00008 0.00001 0.00133 0.      0.      0.\n",
      " 0.00019 0.00135 0.00261 0.001   0.0011  0.00028 0.      0.00016 0.\n",
      " 0.00055 0.      0.00084 0.      0.      0.00247 0.0004  0.      0.\n",
      " 0.      0.      0.      0.00068 0.      0.      0.00165 0.      0.\n",
      " 0.      0.      0.00165 0.00113 0.00199 0.00115 0.      0.00389 0.\n",
      " 0.      0.      0.      0.      0.00071 0.      0.00268 0.00009 0.00001\n",
      " 0.00043 0.00225 0.00015 0.      0.00059 0.00174 0.      0.      0.\n",
      " 0.01049 0.      0.00064 0.00429 0.      0.00315 0.00029 0.00112 0.\n",
      " 0.      0.0001  0.00599 0.00197 0.00052 0.00359 0.00053 0.00366 0.00172\n",
      " 0.00084 0.      0.0002  0.0075  0.00153 0.00139 0.0009  0.00159 0.0011\n",
      " 0.00245 0.00104 0.      0.00431 0.00372 0.00094 0.      0.00058 0.00121\n",
      " 0.      0.00001 0.      0.00037 0.      0.00166 0.00658 0.00003 0.00031\n",
      " 0.00235 0.00152 0.      0.00433 0.00008 0.00028 0.      0.      0.\n",
      " 0.      0.00105 0.      0.      0.      0.      0.0004  0.      0.\n",
      " 0.      0.      0.00054 0.00001 0.      0.00492 0.00258 0.      0.00164\n",
      " 0.00009 0.      0.      0.      0.      0.00392 0.      0.0013  0.00167\n",
      " 0.      0.      0.      0.      0.01505 0.      0.      0.      0.00178\n",
      " 0.00001 0.00181 0.      0.00108 0.00033 0.00142 0.00271 0.      0.0156\n",
      " 0.00188 0.      0.      0.00158 0.00353 0.00005 0.      0.00067 0.\n",
      " 0.00222 0.00338 0.      0.      0.00074 0.00591 0.00155 0.      0.00126\n",
      " 0.00109 0.00457 0.00012 0.00071 0.      0.      0.      0.00128 0.\n",
      " 0.      0.00049 0.      0.      0.00017 0.00445 0.      0.00056 0.00006\n",
      " 0.00006 0.      0.00166 0.      0.0032  0.      0.      0.00007 0.\n",
      " 0.00064 0.00081 0.      0.00069 0.      0.      0.      0.      0.00346\n",
      " 0.      0.00162 0.00006 0.      0.01027 0.00454 0.00729 0.01035 0.00194\n",
      " 0.00131 0.0052  0.      0.      0.      0.      0.      0.00079 0.00033\n",
      " 0.00007 0.00206 0.00151 0.      0.      0.      0.      0.01431 0.00084\n",
      " 0.00186 0.00177 0.00938 0.      0.      0.      0.00116 0.00329 0.\n",
      " 0.      0.      0.      0.0003  0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.00073 0.      0.00056 0.00047\n",
      " 0.0023  0.      0.00082 0.00196 0.      0.00001 0.00054 0.00184 0.\n",
      " 0.00168 0.0001  0.00004 0.      0.      0.0035  0.00282 0.      0.00278\n",
      " 0.      0.00025 0.      0.      0.00139 0.      0.      0.      0.\n",
      " 0.      0.      0.00004 0.      0.      0.      0.00182 0.00009 0.\n",
      " 0.0006  0.00426 0.00006 0.002   0.      0.      0.      0.0003  0.00311\n",
      " 0.00077 0.00441 0.00089 0.      0.00197 0.0068  0.00592 0.00101 0.\n",
      " 0.00034 0.      0.00021 0.00044 0.      0.      0.      0.      0.\n",
      " 0.      0.01293 0.00541 0.      0.00009 0.      0.      0.00096 0.00001\n",
      " 0.00733 0.00075 0.      0.00108 0.      0.00459 0.      0.      0.0004\n",
      " 0.      0.00357 0.00143 0.00292 0.      0.00002 0.      0.00015 0.\n",
      " 0.00102 0.      0.      0.      0.00101 0.00021 0.      0.      0.\n",
      " 0.      0.00085 0.00147 0.00036 0.      0.      0.      0.00045 0.00081\n",
      " 0.      0.00002 0.00106 0.      0.0001  0.      0.      0.      0.00158\n",
      " 0.00151 0.00026 0.      0.00008 0.00034 0.00814 0.      0.00036 0.00084\n",
      " 0.      0.00004 0.00062 0.      0.00035 0.      0.00006 0.      0.00006\n",
      " 0.      0.      0.00196 0.      0.      0.0003  0.00026 0.00033 0.00016\n",
      " 0.00011 0.      0.00528 0.0192  0.00659 0.      0.      0.00018 0.00006\n",
      " 0.00003 0.      0.      0.00137 0.00028 0.      0.0009  0.00007 0.00635\n",
      " 0.00002 0.00071 0.00969 0.00175 0.00077 0.      0.0012  0.      0.0015\n",
      " 0.      0.00488 0.00393 0.      0.00012 0.      0.      0.      0.00012\n",
      " 0.      0.      0.00017 0.00432 0.00023 0.00122 0.      0.00027 0.00006\n",
      " 0.00001 0.      0.      0.00122 0.00169 0.00014 0.00088 0.00001 0.\n",
      " 0.      0.      0.      0.      0.00035 0.00059 0.      0.      0.\n",
      " 0.003   0.      0.      0.      0.      0.      0.00015 0.00552 0.\n",
      " 0.00054 0.00044 0.      0.00109 0.      0.00129 0.00258 0.      0.00292\n",
      " 0.      0.00001 0.00408 0.      0.00253 0.00307 0.00085 0.      0.00156\n",
      " 0.00908 0.      0.      0.0004  0.      0.      0.00667 0.      0.\n",
      " 0.00079 0.00007 0.00229 0.00001 0.00193 0.00107 0.00069 0.      0.00001\n",
      " 0.      0.00007 0.      0.      0.      0.03297 0.00937 0.00368 0.00329\n",
      " 0.0028 ]\n"
     ]
    }
   ],
   "source": [
    "print(topic_word_distributions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, usually people do something like looking at the most probable words per topic, and try to use these words to interpret what the different topics correspond to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying the top 20 words per topic and their probabilities within the topic...\n",
      "\n",
      "[Topic 0]\n",
      "year : 0.03297118094738992\n",
      "team : 0.019204666343880843\n",
      "good : 0.016830138855415535\n",
      "gun : 0.016178567835481206\n",
      "new : 0.015603672962549789\n",
      "mr : 0.01505081138184353\n",
      "president : 0.01431129643505745\n",
      "games : 0.014180220098801988\n",
      "season : 0.012931131385430669\n",
      "league : 0.010491378550696718\n",
      "players : 0.010349770248161931\n",
      "play : 0.010274396314227859\n",
      "hockey : 0.009974003061194872\n",
      "time : 0.009694174123867904\n",
      "best : 0.009409285751708664\n",
      "price : 0.009381674293086522\n",
      "years : 0.009373344778562082\n",
      "win : 0.009083391252969808\n",
      "stephanopoulos : 0.008137787607873974\n",
      "got : 0.007609708926778402\n",
      "\n",
      "[Topic 1]\n",
      "edu : 0.04029110605206494\n",
      "file : 0.03401605842663014\n",
      "com : 0.0222023046148782\n",
      "ftp : 0.014891870249451676\n",
      "available : 0.014872344410171648\n",
      "program : 0.013900099687007832\n",
      "files : 0.01346941290100786\n",
      "mail : 0.012731186252528216\n",
      "list : 0.012302522449955312\n",
      "server : 0.012191608776669492\n",
      "pub : 0.011781900926012206\n",
      "send : 0.011618081633255326\n",
      "information : 0.011328191056635236\n",
      "email : 0.01081941950112364\n",
      "faq : 0.010338836510401675\n",
      "use : 0.010135228627400551\n",
      "anonymous : 0.0096746685473591\n",
      "entry : 0.009404082662349716\n",
      "source : 0.008823763832099402\n",
      "sun : 0.00874015157493989\n",
      "\n",
      "[Topic 2]\n",
      "space : 0.017520829980870203\n",
      "government : 0.014764857355699977\n",
      "law : 0.013984172594173403\n",
      "public : 0.012742615885066314\n",
      "new : 0.011835560307473868\n",
      "university : 0.011455663004367143\n",
      "use : 0.010558752454925957\n",
      "information : 0.010523671907643292\n",
      "national : 0.010005648977110933\n",
      "research : 0.009837721562709864\n",
      "state : 0.009253714677482205\n",
      "states : 0.008693409057582443\n",
      "data : 0.00813717432781462\n",
      "general : 0.007869014386848885\n",
      "1993 : 0.007725106577330434\n",
      "privacy : 0.007621978472840703\n",
      "nasa : 0.007618877183926805\n",
      "control : 0.007514320482818127\n",
      "center : 0.007103239931985117\n",
      "technology : 0.006967116433727473\n",
      "\n",
      "[Topic 3]\n",
      "people : 0.025895408882525735\n",
      "don : 0.019051730185797967\n",
      "just : 0.016890775051117413\n",
      "think : 0.014747845951330904\n",
      "know : 0.014199565695037608\n",
      "like : 0.014113213308107714\n",
      "said : 0.010874414405776101\n",
      "time : 0.010682271372211054\n",
      "right : 0.010536202795306688\n",
      "did : 0.009462650925254166\n",
      "say : 0.008821296230055444\n",
      "going : 0.007917870731667824\n",
      "want : 0.007522594449012154\n",
      "ve : 0.007291645923099386\n",
      "way : 0.007098601161894282\n",
      "didn : 0.007067349290356842\n",
      "make : 0.006286341816011715\n",
      "really : 0.006220033365741903\n",
      "years : 0.006030323009314076\n",
      "ll : 0.005758539222688519\n",
      "\n",
      "[Topic 4]\n",
      "10 : 0.04722606222503568\n",
      "00 : 0.03326765379925959\n",
      "11 : 0.031173500801189198\n",
      "12 : 0.030123700377868004\n",
      "15 : 0.02990359767341554\n",
      "25 : 0.029398719591354996\n",
      "20 : 0.028711218264319927\n",
      "14 : 0.026568760431583533\n",
      "16 : 0.024643647803909676\n",
      "17 : 0.023216760421633384\n",
      "13 : 0.02289261427726245\n",
      "18 : 0.020309457032141442\n",
      "24 : 0.01874689898563933\n",
      "40 : 0.01741601274278343\n",
      "30 : 0.017264702358222795\n",
      "55 : 0.017249979067923173\n",
      "19 : 0.016906732935191186\n",
      "21 : 0.01649286991993608\n",
      "23 : 0.015481325326843542\n",
      "22 : 0.015325721360768468\n",
      "\n",
      "[Topic 5]\n",
      "windows : 0.027479651671251874\n",
      "db : 0.020081302606172316\n",
      "software : 0.019461723980915148\n",
      "dos : 0.01664587139692074\n",
      "card : 0.01662946225244954\n",
      "image : 0.014905777621753494\n",
      "disk : 0.014882547183197302\n",
      "graphics : 0.014394671626531581\n",
      "data : 0.014324999278027262\n",
      "pc : 0.012852843991338195\n",
      "color : 0.012760377141738092\n",
      "mac : 0.012521671837075712\n",
      "memory : 0.012288066764058235\n",
      "window : 0.011725439246988113\n",
      "version : 0.011359671702651977\n",
      "use : 0.011144096720246771\n",
      "display : 0.01070306970233045\n",
      "using : 0.010212944450226597\n",
      "bit : 0.010158024468027209\n",
      "screen : 0.009825308617938965\n",
      "\n",
      "[Topic 6]\n",
      "key : 0.036657227264248915\n",
      "thanks : 0.033309192242443476\n",
      "know : 0.027546143029699155\n",
      "does : 0.023024362368053008\n",
      "chip : 0.020030336089088198\n",
      "use : 0.01733602330954036\n",
      "encryption : 0.017136837158848565\n",
      "help : 0.016496899538802737\n",
      "like : 0.016002653922314588\n",
      "mail : 0.015804000270816323\n",
      "need : 0.015344271069585632\n",
      "keys : 0.01374776217804683\n",
      "looking : 0.013312665254122537\n",
      "clipper : 0.012823434108060605\n",
      "used : 0.012362006854300478\n",
      "sound : 0.012103168531042697\n",
      "hi : 0.011949674369115254\n",
      "advance : 0.010790313074703371\n",
      "information : 0.010637453739816572\n",
      "bit : 0.0102371342314524\n",
      "\n",
      "[Topic 7]\n",
      "god : 0.036778972973209936\n",
      "jesus : 0.017553829079132426\n",
      "does : 0.015816424138174422\n",
      "believe : 0.013958023130356151\n",
      "game : 0.01217174589515135\n",
      "people : 0.011042810079440597\n",
      "say : 0.011006544304419672\n",
      "christian : 0.010853081080413905\n",
      "true : 0.01063009302542626\n",
      "bible : 0.01033931356085632\n",
      "think : 0.009774171445882909\n",
      "church : 0.009668704701033187\n",
      "life : 0.008833494899372603\n",
      "way : 0.00785269950643299\n",
      "religion : 0.007590975235037148\n",
      "christians : 0.0075449880919840175\n",
      "christ : 0.007539616094334565\n",
      "faith : 0.007439660543789094\n",
      "point : 0.007427660316865333\n",
      "good : 0.0071864568563567025\n",
      "\n",
      "[Topic 8]\n",
      "drive : 0.02229573986430297\n",
      "power : 0.019507341681994316\n",
      "like : 0.018522559790376477\n",
      "just : 0.016924517655010525\n",
      "car : 0.016675980375469995\n",
      "use : 0.01546275692548463\n",
      "scsi : 0.013950686273385776\n",
      "ve : 0.013925867683506213\n",
      "good : 0.011218067098463956\n",
      "speed : 0.011115387347522961\n",
      "hard : 0.010994071990946796\n",
      "used : 0.010715407919410558\n",
      "don : 0.01021285486454343\n",
      "problem : 0.010109223796813019\n",
      "work : 0.009567260783500339\n",
      "drives : 0.00822283821010156\n",
      "buy : 0.008009702543935435\n",
      "better : 0.007788251352635239\n",
      "high : 0.00775831079830328\n",
      "does : 0.0073133797599765175\n",
      "\n",
      "[Topic 9]\n",
      "ax : 0.7750508616859831\n",
      "max : 0.05676346028145709\n",
      "g9v : 0.017637993832254468\n",
      "b8f : 0.015315278818004917\n",
      "a86 : 0.012646093392057575\n",
      "145 : 0.010168944298400002\n",
      "pl : 0.01012365428397651\n",
      "1d9 : 0.008174180919536261\n",
      "1t : 0.006531951660828925\n",
      "0t : 0.006459809882532865\n",
      "bhj : 0.006110218348574932\n",
      "giz : 0.005453049410747112\n",
      "3t : 0.005447285579836965\n",
      "34u : 0.005285937363655366\n",
      "2di : 0.005090173874145282\n",
      "75u : 0.004632619379523059\n",
      "wm : 0.004518731940833898\n",
      "2tm : 0.004222775559250272\n",
      "7ey : 0.0036648191849338423\n",
      "bxn : 0.0032500074436691783\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_top_words = 20\n",
    "\n",
    "print('Displaying the top %d words per topic and their probabilities within the topic...' % num_top_words)\n",
    "print()\n",
    "\n",
    "for topic_idx in range(num_topics):\n",
    "    print('[Topic ', topic_idx, ']', sep='')\n",
    "    sort_indices = np.argsort(-topic_word_distributions[topic_idx])\n",
    "    for rank in range(num_top_words):\n",
    "        word_idx = sort_indices[rank]\n",
    "        print(tf_vectorizer.get_feature_names()[word_idx], ':', topic_word_distributions[topic_idx, word_idx])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `transform()` function to figure out for each document, what fraction of it is explained by each of the topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_matrix = lda.transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topic_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00182, 0.00182, 0.00182, 0.00182, 0.00182, 0.00182, 0.00182,\n",
       "       0.82791, 0.15754, 0.00182])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topic_matrix[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing co-occurrences of words\n",
    "\n",
    "Here, we count the number of newsgroup posts in which two words both occur. This part of the demo should feel like a review of co-occurrence analysis from earlier in the course, except now we use scikit-learn's built-in CountVectorizer. Conceptually everything else in the same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word1 = 'year'\n",
    "word2 = 'team'\n",
    "\n",
    "word1_column_idx = tf_vectorizer.vocabulary_[word1]\n",
    "word2_column_idx = tf_vectorizer.vocabulary_[word2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(tf.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf[:, word1_column_idx].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_with_word1 = (tf[:, word1_column_idx].toarray().flatten() > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_with_word2 = (tf[:, word2_column_idx].toarray().flatten() > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_with_both_word1_and_word2 = documents_with_word1 * documents_with_word2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compute the log of the conditional probability of word 1 appearing given that word 2 appeared, where we add in a little bit of a fudge factor in the numerator (in this case, it's actually not needed but some times you do have two words that do not co-occur for which you run into a numerical issue due to taking the log of 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.1\n",
    "np.log2((documents_with_both_word1_and_word2.sum() + eps) / documents_with_word2.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_see_word1_given_see_word2(word1, word2, vectorizer, eps=0.1):\n",
    "    word1_column_idx = vectorizer.vocabulary_[word1]\n",
    "    word2_column_idx = vectorizer.vocabulary_[word2]\n",
    "    documents_with_word1 = (tf[:, word1_column_idx].toarray().flatten() > 0)\n",
    "    documents_with_word2 = (tf[:, word2_column_idx].toarray().flatten() > 0)\n",
    "    documents_with_both_word1_and_word2 = documents_with_word1 * documents_with_word2\n",
    "    return np.log2((documents_with_both_word1_and_word2.sum() + eps) / documents_with_word2.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic coherence\n",
    "\n",
    "The below code shows how one implements the topic coherence calculation from lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_coherence = 0\n",
    "for topic_idx in range(num_topics):\n",
    "    print('[Topic ', topic_idx, ']', sep='')\n",
    "    sort_indices = np.argsort(topic_word_distributions[topic_idx])[::-1]\n",
    "    coherence = 0.\n",
    "    for top_word_idx1 in sort_indices[:num_top_words]:\n",
    "        word1 = tf_vectorizer.get_feature_names()[top_word_idx1]\n",
    "        for top_word_idx2 in sort_indices[:num_top_words]:\n",
    "            word2 = tf_vectorizer.get_feature_names()[top_word_idx2]\n",
    "            if top_word_idx1 != top_word_idx2:\n",
    "                coherence += prob_see_word1_given_see_word2(word1, word2, tf_vectorizer, 0.1)\n",
    "    print('Coherence:', coherence)\n",
    "    print()\n",
    "    average_coherence += coherence\n",
    "average_coherence /= num_topics\n",
    "print('Average coherence:', average_coherence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of unique words\n",
    "\n",
    "The below code shows how one implements the number of unique words calculation from lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_number_of_unique_top_words = 0\n",
    "for topic_idx1 in range(num_topics):\n",
    "    print('[Topic ', topic_idx1, ']', sep='')\n",
    "    sort_indices1 = np.argsort(topic_word_distributions[topic_idx1])[::-1]\n",
    "    num_unique_top_words = 0\n",
    "    for top_word_idx1 in sort_indices1[:num_top_words]:\n",
    "        word1 = tf_vectorizer.get_feature_names()[top_word_idx1]\n",
    "        break_ = False\n",
    "        for topic_idx2 in range(num_topics):\n",
    "            if topic_idx1 != topic_idx2:\n",
    "                sort_indices2 = np.argsort(topic_word_distributions[topic_idx2])[::-1]\n",
    "                for top_word_idx2 in sort_indices2[:num_top_words]:\n",
    "                    word2 = tf_vectorizer.get_feature_names()[top_word_idx2]\n",
    "                    if word1 == word2:\n",
    "                        break_ = True\n",
    "                        break\n",
    "                if break_:\n",
    "                    break\n",
    "        else:\n",
    "            num_unique_top_words += 1\n",
    "    print('Number of unique top words:', num_unique_top_words)\n",
    "    print()\n",
    "    \n",
    "    average_number_of_unique_top_words += num_unique_top_words\n",
    "average_number_of_unique_top_words /= num_topics\n",
    "print('Average number of unique top words:', average_number_of_unique_top_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
