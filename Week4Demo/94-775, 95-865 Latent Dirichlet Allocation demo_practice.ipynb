{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 94-775/95-865: Topic Modeling with Latent Dirichlet Allocation\n",
    "\n",
    "Author: George H. Chen (georgechen [at symbol] cmu.edu)\n",
    "\n",
    "The beginning part of this demo is a shortened and modified version of sklearn's LDA & NMF demo (http://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=5, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load in 10,000 posts from the 20 Newsgroups dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "num_articles = 10000\n",
    "data = fetch_20newsgroups(shuffle=True, random_state=0,\n",
    "                          remove=('headers', 'footers', 'quotes')).data[:num_articles]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that there are 10,000 posts, and we can look at an example post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Experimental Lyme Disease in Dogs Produces Arthritis and Persistant Infection,\n",
      "The Journal of Infectious Diseases, March 1993, 167:651-664\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# you can take a look at what individual documents look like by replacing what index we look at\n",
    "print(data[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now fit a `CountVectorizer` model that will compute, for each post, what its raw word count histograms are (the \"term frequencies\" we saw in week 1).\n",
    "\n",
    "The output of the following cell is the term-frequencies matrix, where rows index different posts/text documents, and columns index 1000 different vocabulary words. A note about the arguments to `CountVectorizer`:\n",
    "\n",
    "- `max_df`: we only keep words that appear in at most this fraction of the documents\n",
    "- `min_df`: we only keep words that appear in at least this many documents\n",
    "- `stop_words`: whether to remove stop words\n",
    "- `max_features`: among words that don't get removed due to the above 3 arguments, we keep the top `max_features` number of most frequently occuring words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# CountVectorizer does tokenization and can remove terms that occur too frequently, not frequently enough, or that are stop words\n",
    "\n",
    "# document frequency (df) means number of documents a word appears in\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95,\n",
    "                                min_df=2,\n",
    "                                stop_words='english',\n",
    "                                max_features=vocab_size)\n",
    "tf = tf_vectorizer.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that there are 10,000 rows (corresponding to posts), and 1000 columns (corresponding to words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A note about the `tf` matrix: this actually is stored as what's called a sparse matrix (rather than a 2D NumPy array that you're more familiar with). The reason is that often these matrices are really large and the vast majority of entries are 0, so it's possible to save space by not storing where the 0's are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tf)# to save space, matrix only store none 0 values in it. most value in matrix is 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " To convert `tf` to a 2D NumPy table, you can run `tf.toarray()` (this does not modify the original `tf` variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.toarray().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can figure out what words the different columns correspond to by using the `get_feature_names()` function; the output is in the same order as the column indices. In particular, we can index into the following list (i.e., so given a column index, we can figure out which word it corresponds to)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '02', '03', '04', '0d', '0t', '10', '100', '11', '12', '128', '13', '14', '145', '15', '16', '17', '18', '19', '1990', '1991', '1992', '1993', '1d9', '1st', '1t', '20', '200', '21', '22', '23', '24', '25', '250', '26', '27', '28', '29', '2di', '2tm', '30', '300', '31', '32', '33', '34', '34u', '35', '36', '37', '38', '39', '3d', '3t', '40', '41', '42', '43', '44', '45', '46', '48', '50', '500', '55', '60', '64', '6um', '70', '75', '75u', '7ey', '80', '800', '86', '90', '91', '92', '93', '9v', 'a86', 'ability', 'able', 'ac', 'accept', 'access', 'according', 'act', 'action', 'actually', 'add', 'addition', 'address', 'administration', 'advance', 'age', 'ago', 'agree', 'ah', 'air', 'al', 'algorithm', 'allow', 'allowed', 'alt', 'america', 'american', 'analysis', 'anonymous', 'answer', 'answers', 'anti', 'anybody', 'apparently', 'appears', 'apple', 'application', 'applications', 'appreciate', 'appreciated', 'approach', 'appropriate', 'apr', 'april', 'archive', 'area', 'areas', 'aren', 'argument', 'armenia', 'armenian', 'armenians', 'arms', 'army', 'article', 'articles', 'ask', 'asked', 'asking', 'assume', 'atheism', 'attack', 'attempt', 'au', 'author', 'authority', 'available', 'average', 'avoid', 'away', 'ax', 'b8f', 'bad', 'base', 'based', 'basic', 'basically', 'basis', 'belief', 'believe', 'best', 'better', 'bh', 'bhj', 'bible', 'big', 'bike', 'bit', 'bits', 'bj', 'black', 'block', 'blood', 'board', 'body', 'book', 'books', 'bought', 'box', 'break', 'bring', 'brought', 'btw', 'buf', 'build', 'building', 'built', 'bus', 'business', 'buy', 'bxn', 'ca', 'cable', 'california', 'called', 'calls', 'came', 'canada', 'car', 'card', 'cards', 'care', 'carry', 'cars', 'case', 'cases', 'cause', 'cd', 'center', 'certain', 'certainly', 'chance', 'change', 'changed', 'changes', 'check', 'chicago', 'child', 'children', 'chip', 'chips', 'choice', 'christ', 'christian', 'christianity', 'christians', 'church', 'citizens', 'city', 'claim', 'claims', 'class', 'clear', 'clearly', 'clinton', 'clipper', 'close', 'code', 'color', 'com', 'come', 'comes', 'coming', 'command', 'comments', 'commercial', 'committee', 'common', 'community', 'comp', 'company', 'complete', 'completely', 'computer', 'condition', 'conference', 'congress', 'consider', 'considered', 'contact', 'contains', 'context', 'continue', 'control', 'controller', 'copy', 'correct', 'cost', 'couldn', 'country', 'couple', 'course', 'court', 'cover', 'create', 'created', 'crime', 'cross', 'cs', 'current', 'currently', 'cut', 'cx', 'data', 'date', 'dave', 'david', 'day', 'days', 'db', 'dc', 'dead', 'deal', 'death', 'dec', 'decided', 'defense', 'define', 'deleted', 'department', 'des', 'design', 'designed', 'details', 'development', 'device', 'devices', 'did', 'didn', 'difference', 'different', 'difficult', 'digital', 'directly', 'directory', 'discussion', 'disk', 'display', 'distribution', 'division', 'dod', 'does', 'doesn', 'doing', 'don', 'door', 'dos', 'doubt', 'dr', 'drive', 'driver', 'drivers', 'drives', 'drug', 'early', 'earth', 'easily', 'east', 'easy', 'ed', 'edu', 'effect', 'electronic', 'email', 'encryption', 'end', 'enforcement', 'engine', 'entire', 'entry', 'environment', 'error', 'escrow', 'especially', 'event', 'events', 'evidence', 'exactly', 'example', 'excellent', 'exist', 'existence', 'exists', 'expect', 'experience', 'explain', 'export', 'extra', 'face', 'fact', 'faith', 'false', 'family', 'faq', 'far', 'fast', 'faster', 'father', 'fax', 'fbi', 'features', 'federal', 'feel', 'field', 'figure', 'file', 'files', 'final', 'finally', 'fine', 'firearms', 'floppy', 'folks', 'follow', 'following', 'food', 'force', 'form', 'format', 'free', 'freedom', 'friend', 'ftp', 'function', 'functions', 'future', 'g9v', 'game', 'games', 'gas', 'gave', 'general', 'generally', 'gets', 'getting', 'gif', 'given', 'gives', 'giz', 'gk', 'gm', 'goal', 'god', 'goes', 'going', 'good', 'got', 'gov', 'government', 'graphics', 'great', 'greek', 'ground', 'group', 'groups', 'guess', 'gun', 'guns', 'guy', 'half', 'hand', 'happen', 'happened', 'happens', 'hard', 'hardware', 'haven', 'having', 'head', 'health', 'hear', 'heard', 'held', 'hell', 'help', 'hi', 'high', 'higher', 'history', 'hit', 'hockey', 'hold', 'home', 'hope', 'hours', 'house', 'hp', 'human', 'ibm', 'ide', 'idea', 'ideas', 'ii', 'image', 'images', 'imagine', 'important', 'include', 'included', 'includes', 'including', 'individual', 'info', 'information', 'input', 'inside', 'installed', 'instead', 'insurance', 'int', 'interested', 'interesting', 'interface', 'internal', 'international', 'internet', 'involved', 'isn', 'israel', 'israeli', 'issue', 'issues', 'jesus', 'jewish', 'jews', 'jim', 'job', 'jobs', 'john', 'jpeg', 'just', 'key', 'keyboard', 'keys', 'kill', 'killed', 'kind', 'knew', 'know', 'knowledge', 'known', 'knows', 'la', 'land', 'language', 'large', 'late', 'later', 'law', 'laws', 'league', 'learn', 'leave', 'left', 'legal', 'let', 'letter', 'level', 'library', 'life', 'light', 'like', 'likely', 'limited', 'line', 'lines', 'list', 'little', 'live', 'lives', 'living', 'll', 'local', 'long', 'longer', 'look', 'looked', 'looking', 'looks', 'lord', 'lost', 'lot', 'lots', 'love', 'low', 'lower', 'mac', 'machine', 'machines', 'mail', 'main', 'major', 'make', 'makes', 'making', 'man', 'manager', 'manual', 'mark', 'market', 'mass', 'master', 'material', 'matter', 'max', 'maybe', 'mb', 'mean', 'meaning', 'means', 'media', 'medical', 'members', 'memory', 'men', 'mention', 'mentioned', 'message', 'mike', 'miles', 'military', 'million', 'mind', 'mit', 'mode', 'model', 'modem', 'money', 'monitor', 'month', 'months', 'moral', 'mother', 'motif', 'mouse', 'mr', 'ms', 'multiple', 'nasa', 'national', 'nature', 'near', 'necessary', 'need', 'needed', 'needs', 'net', 'network', 'new', 'news', 'newsgroup', 'nhl', 'nice', 'night', 'non', 'normal', 'note', 'nsa', 'number', 'numbers', 'object', 'obvious', 'obviously', 'offer', 'office', 'official', 'oh', 'ok', 'old', 'ones', 'open', 'opinion', 'opinions', 'orbit', 'order', 'org', 'organization', 'original', 'os', 'output', 'outside', 'package', 'page', 'paper', 'particular', 'parts', 'party', 'past', 'paul', 'pay', 'pc', 'peace', 'people', 'perfect', 'performance', 'period', 'person', 'personal', 'phone', 'pick', 'picture', 'pin', 'pittsburgh', 'pl', 'place', 'places', 'plan', 'play', 'played', 'player', 'players', 'plus', 'point', 'points', 'police', 'policy', 'political', 'population', 'port', 'position', 'possible', 'possibly', 'post', 'posted', 'posting', 'power', 'pp', 'present', 'president', 'press', 'pretty', 'previous', 'price', 'printer', 'privacy', 'private', 'pro', 'probably', 'problem', 'problems', 'process', 'product', 'program', 'programs', 'project', 'protect', 'provide', 'provides', 'pub', 'public', 'published', 'purpose', 'qq', 'quality', 'question', 'questions', 'quite', 'radio', 'ram', 'range', 'rate', 'read', 'reading', 'real', 'really', 'reason', 'reasonable', 'reasons', 'received', 'recent', 'recently', 'record', 'red', 'reference', 'regular', 'related', 'release', 'religion', 'religious', 'remember', 'reply', 'report', 'reports', 'request', 'require', 'required', 'requires', 'research', 'resources', 'response', 'rest', 'result', 'results', 'return', 'right', 'rights', 'road', 'rom', 'room', 'round', 'rules', 'run', 'running', 'runs', 'russian', 'safety', 'said', 'sale', 'san', 'save', 'saw', 'say', 'saying', 'says', 'school', 'sci', 'science', 'scientific', 'screen', 'scsi', 'search', 'season', 'second', 'secret', 'section', 'secure', 'security', 'seen', 'self', 'sell', 'send', 'sense', 'sent', 'serial', 'series', 'server', 'service', 'set', 'shall', 'shipping', 'short', 'shot', 'shuttle', 'similar', 'simple', 'simply', 'sin', 'single', 'site', 'sites', 'situation', 'size', 'small', 'society', 'software', 'solution', 'son', 'soon', 'sorry', 'sort', 'sound', 'sounds', 'source', 'sources', 'south', 'soviet', 'space', 'special', 'specific', 'speed', 'spirit', 'st', 'standard', 'start', 'started', 'state', 'statement', 'states', 'station', 'stephanopoulos', 'steve', 'stop', 'story', 'stream', 'street', 'strong', 'study', 'stuff', 'subject', 'suggest', 'sun', 'support', 'supports', 'supposed', 'sure', 'systems', 'taken', 'takes', 'taking', 'talk', 'talking', 'tape', 'tar', 'tax', 'team', 'teams', 'technical', 'technology', 'tell', 'term', 'terms', 'test', 'text', 'thank', 'thanks', 'theory', 'thing', 'things', 'think', 'thinking', 'thought', 'time', 'times', 'title', 'tm', 'today', 'told', 'took', 'tools', 'total', 'trade', 'transfer', 'tried', 'true', 'truth', 'try', 'trying', 'turkey', 'turkish', 'turn', 'tv', 'type', 'uk', 'understand', 'unfortunately', 'unit', 'united', 'university', 'unix', 'unless', 'usa', 'use', 'used', 'useful', 'usenet', 'user', 'users', 'uses', 'using', 'usually', 'value', 'values', 'van', 'various', 've', 'version', 'vga', 'video', 'view', 'voice', 'volume', 'vs', 'wait', 'want', 'wanted', 'wants', 'war', 'washington', 'wasn', 'watch', 'water', 'way', 'ways', 'weapons', 'week', 'weeks', 'went', 'white', 'wide', 'widget', 'willing', 'win', 'window', 'windows', 'wish', 'wm', 'women', 'won', 'word', 'words', 'work', 'worked', 'working', 'works', 'world', 'worth', 'wouldn', 'write', 'writing', 'written', 'wrong', 'wrote', 'x11', 'xt', 'year', 'years', 'yes', 'york', 'young']\n"
     ]
    }
   ],
   "source": [
    "print(tf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also go in reverse: given a word, we can figure out which column index it corresponds to. To do this, we use the `vocabulary_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_vectorizer.vocabulary_['book']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can figure out what the raw counts are for the 0-th post as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 2, 3, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf[0].toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now fit an LDA model to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                          evaluate_every=-1, learning_decay=0.7,\n",
       "                          learning_method='online', learning_offset=10.0,\n",
       "                          max_doc_update_iter=100, max_iter=10,\n",
       "                          mean_change_tol=0.001, n_components=10, n_jobs=None,\n",
       "                          perp_tol=0.1, random_state=0, topic_word_prior=None,\n",
       "                          total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_topics = 10\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, learning_method='online', random_state=0)\n",
    "lda.fit(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fitting procedure determines the every topic's distribution over words; this information is stored in the `components_` attribute. There's a catch: we actually have to normalize to get the probability distributions (without this normalization, instead what the model has are pseudocounts for how often different words appear per topic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1000)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1000)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 36546.2359 ,  42637.17064,  49198.91281, 114036.11143,\n",
       "        25195.09295,  38394.6771 ,  28563.10527,  47579.92792,\n",
       "        39951.26973,  63662.21236])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 127.68851,    0.10004,    0.10002,    0.10001,    0.10005,\n",
       "          0.1    ,    0.1    ,  111.30353,  230.73776,    0.10964,\n",
       "         41.27432,    0.10001,    4.25971,    0.28019,    0.10001,\n",
       "         64.39541,    0.10405,    0.10866,    0.10084,    0.10461,\n",
       "         71.83736,   51.52613,  116.22922,   26.99287,    0.1    ,\n",
       "         83.10313,    0.1    ,  113.32924,    0.10004,    0.10006,\n",
       "          0.6443 ,    0.10031,    0.10005,    7.8153 ,   36.53749,\n",
       "          0.10022,    0.10002,    0.101  ,    0.10427,    0.1    ,\n",
       "          0.1    ,  104.0695 ,  120.15554,    0.11544,    0.10038,\n",
       "          0.10071,    0.15353,    0.1    ,   14.55346,    0.10006,\n",
       "          0.10006,    0.10006,    0.10133,    0.10001,    0.1    ,\n",
       "         60.73281,    0.10304,    0.10191,    0.10079,    0.10001,\n",
       "         15.16448,    0.10047,    0.17649,  225.63527,  139.07519,\n",
       "          0.10095,   89.40049,    0.10003,    0.1    ,   37.72724,\n",
       "         40.99187,    0.1    ,    0.1    ,   14.85706,   45.3433 ,\n",
       "          0.10003,   75.74785,   47.69516,   52.47337,   18.24178,\n",
       "          0.1    ,    0.1    ,    0.10005,   21.59127,    0.10003,\n",
       "          7.38839,    0.10001,    0.10004,    0.10035,   30.86552,\n",
       "         76.88971,   39.22284,    0.10059,    0.10003,    2.8419 ,\n",
       "          8.64021,   46.6746 ,   77.07516,    0.10057,    0.10002,\n",
       "          0.10003,    0.10006,    0.10001,    0.10003,   42.26643,\n",
       "          0.10011,  131.52916,  104.70815,    0.10006,    0.10001,\n",
       "          0.10362,    0.10002,    0.10021,    0.10004,   14.05281,\n",
       "         88.91554,    0.10003,    0.10002,    0.10001,    0.10029,\n",
       "          0.10008,    0.10408,    0.10005,    0.10024,   33.89311,\n",
       "          0.10001,   32.65328,    6.93798,   11.03119,    0.10059,\n",
       "          0.1    ,    0.1    ,    0.1    ,    0.10092,    0.10004,\n",
       "          0.10002,    0.10004,    5.43549,    0.10005,  129.19363,\n",
       "          0.10156,    0.1    ,    0.10045,    0.10003,    0.10001,\n",
       "          0.10002,    0.10001,    0.39501,  229.45231,    0.10002,\n",
       "         62.47434,    0.1    ,    0.1    ,   97.53776,   93.2756 ,\n",
       "         33.8783 ,    2.77402,   13.68931,    0.10084,    0.10001,\n",
       "         22.19548,  343.87398,  198.88239,    0.1    ,    0.1    ,\n",
       "          0.10001,  103.84239,    0.10004,   28.85985,    0.10001,\n",
       "          0.10001,  109.21016,    0.10019,    0.10002,   10.64697,\n",
       "          7.35578,   39.06748,    6.86665,   30.66127,   45.73691,\n",
       "         20.56406,   23.73328,    0.10003,   13.06944,    0.10003,\n",
       "          0.10004,    0.12608,    0.10432,    0.10001,   30.55066,\n",
       "         81.115  ,    0.1    ,   55.79826,   27.76808,   62.06912,\n",
       "         14.61728,    0.10004,   76.86302,  177.83943,    0.10002,\n",
       "          0.10003,   25.84555,   36.80984,   11.69097,    0.10013,\n",
       "         30.6377 ,    6.41498,    0.10113,    0.10008,   87.10866,\n",
       "          1.27331,    0.10006,   77.8668 ,   15.27706,   32.23478,\n",
       "          8.541  ,   71.28206,    0.10009,    0.10002,    0.10003,\n",
       "          0.10027,    0.1061 ,   29.6285 ,    0.1    ,    0.10001,\n",
       "          0.1    ,    0.10001,    0.1    ,    0.10252,  170.99144,\n",
       "          0.10002,    0.14097,    0.10005,   10.22634,    0.10041,\n",
       "         43.30438,    0.1    ,    0.10009,    0.10001,    0.10001,\n",
       "          0.15811,   72.56323,   33.03333,   50.62959,    0.10001,\n",
       "         41.05512,    0.10002,    0.10017,    0.10515,    0.11334,\n",
       "          0.10001,    0.16454,   26.9158 ,    3.10718,    0.10015,\n",
       "        189.79033,   88.96446,    2.67767,   70.6306 ,   28.52747,\n",
       "         54.00844,    0.10003,    0.10003,   30.83691,   45.23161,\n",
       "          0.10002,   28.74091,    0.2093 ,   27.54622,    0.1001 ,\n",
       "         11.92349,    0.10008,   39.64703,    0.10182,  179.94012,\n",
       "          2.97634,    0.10003,    0.30736,   31.223  ,    0.1001 ,\n",
       "         21.58358,   36.86285,   18.556  ,    0.1    ,    0.10087,\n",
       "         40.16948,  146.841  ,    0.10004,  152.91722,   55.04059,\n",
       "          0.1    ,   35.96598,    7.77744,   87.17841,    0.10004,\n",
       "          0.10005,    0.10009,  145.29816,    0.10002,    3.7673 ,\n",
       "          5.4762 ,    0.1    ,    2.97415,    0.1005 ,    0.10003,\n",
       "          0.10006,    0.10055,    0.10002,  131.85521,   86.92765,\n",
       "         28.20227,    7.86513,    1.59508,    0.10001,    0.10004,\n",
       "          0.10001,    0.11571,    0.10001,    0.10002,    0.10001,\n",
       "        161.25199,  169.61595,   10.81906,   36.59228,   14.30571,\n",
       "        172.47374,    0.10003,    0.1    ,    0.10004,   17.11266,\n",
       "          0.10001,    0.10002,    0.10003,    0.10001,    0.22901,\n",
       "        106.09101,    0.10128,    0.10003,   49.67254,    0.10593,\n",
       "         63.41439,  124.30838,   22.40322,    0.10007,    0.10005,\n",
       "          0.1    ,  162.83061,    0.10024,    0.10001,   12.89651,\n",
       "          0.1001 ,    0.10004,    0.10004,    0.1    ,    6.83951,\n",
       "         12.55922,    0.1022 ,    0.10002,    6.03229,    0.15257,\n",
       "        184.4401 ,    0.10002,    0.10001,    0.10001,   57.92664,\n",
       "          2.97924,    0.10002,    0.10002,   62.7412 ,   23.3511 ,\n",
       "          1.33936,    0.10002,    0.10001,    0.10002,    0.1    ,\n",
       "        101.9987 ,    0.10003,    0.10033,    0.10001,    0.10003,\n",
       "          0.10001,    0.10002,    0.10001,   57.0079 ,    0.10007,\n",
       "         20.72003,    0.10002,    0.10001,  127.18387,    0.10006,\n",
       "         12.5506 ,    1.90902,    0.1    ,    4.41222,    6.72081,\n",
       "         63.61871,    0.10001,   62.93494,    0.10009,    0.10001,\n",
       "         88.85565,    0.10001,   62.07107,    0.1    ,    0.10002,\n",
       "          0.10001,   61.04946,    0.1    ,    0.10013,  518.23367,\n",
       "          0.10002,    0.10006,    8.38406,    0.14912,   76.55907,\n",
       "          0.10013,    0.1    ,   47.56419,    0.10007,    0.1    ,\n",
       "          0.1    ,    0.10003,  222.81383,    0.1    ,   73.51433,\n",
       "        157.58155,  615.07822,  278.10622,    0.10001,    0.10347,\n",
       "          0.1    ,  264.27263,    0.10001,    0.10297,    4.82275,\n",
       "          0.10848,   31.73285,  591.26576,   56.35528,   83.90449,\n",
       "         31.2504 ,    4.16428,    7.75473,    0.1055 ,    7.84498,\n",
       "         51.79536,    0.10061,   27.55356,    4.63048,   40.82024,\n",
       "          8.15215,   10.65331,    0.10003,    0.10005,    0.10026,\n",
       "         12.70829,    0.10001,  113.90304,   44.52294,    0.11396,\n",
       "        199.97571,  364.51227,    0.10003,  170.46277,   57.42219,\n",
       "          0.10004,   58.61108,    0.1001 ,    0.1001 ,    0.10002,\n",
       "          0.1    ,    2.94052,    0.23809,   48.69714,    0.10001,\n",
       "          0.1    ,    0.10002,    6.99151,   49.25471,   95.37214,\n",
       "         36.48092,   40.26285,   10.21088,    0.10002,    5.79287,\n",
       "          0.10003,   20.0767 ,    0.10001,   30.58411,    0.10007,\n",
       "          0.10001,   90.2138 ,   14.43676,    0.1    ,    0.12354,\n",
       "          0.10004,    0.10002,    0.10003,   25.00661,    0.10001,\n",
       "          0.1    ,   60.41346,    0.10018,    0.1    ,    0.10002,\n",
       "          0.1    ,   60.32257,   41.16338,   72.76557,   42.18923,\n",
       "          0.1    ,  142.09107,    0.10002,    0.10001,    0.10015,\n",
       "          0.10287,    0.12083,   25.90641,    0.16227,   98.02369,\n",
       "          3.33234,    0.19961,   15.66743,   82.28801,    5.33544,\n",
       "          0.10003,   21.4937 ,   63.45867,    0.1001 ,    0.10016,\n",
       "          0.10077,  383.4204 ,    0.11072,   23.39762,  156.90854,\n",
       "          0.10001,  115.08704,   10.58049,   40.92196,    0.10001,\n",
       "          0.10116,    3.53518,  218.8604 ,   71.86896,   19.07114,\n",
       "        131.139  ,   19.32657,  133.6076 ,   62.81337,   30.74593,\n",
       "          0.10003,    7.16367,  274.02589,   56.08422,   50.86324,\n",
       "         32.81277,   58.05771,   40.02954,   89.5149 ,   38.12738,\n",
       "          0.10001,  157.58766,  135.95549,   34.51108,    0.10005,\n",
       "         21.36168,   44.27024,    0.10001,    0.28352,    0.10002,\n",
       "         13.4329 ,    0.11839,   60.81713,  240.3856 ,    1.15041,\n",
       "         11.3979 ,   85.83519,   55.47861,    0.10004,  158.26592,\n",
       "          3.05186,   10.09894,    0.1001 ,    0.10005,    0.10051,\n",
       "          0.10005,   38.19406,    0.10001,    0.10004,    0.10004,\n",
       "          0.16638,   14.71704,    0.10006,    0.10019,    0.10004,\n",
       "          0.10261,   19.81432,    0.28069,    0.10002,  179.63473,\n",
       "         94.32778,    0.10001,   60.04226,    3.33177,    0.10001,\n",
       "          0.10001,    0.10003,    0.10001,  143.23831,    0.10001,\n",
       "         47.52398,   60.91895,    0.10001,    0.10004,    0.1    ,\n",
       "          0.10001,  550.0505 ,    0.10007,    0.10001,    0.10001,\n",
       "         64.93143,    0.22954,   66.04098,    0.10043,   39.56205,\n",
       "         11.94216,   51.82428,   99.07653,    0.10006,  570.25551,\n",
       "         68.65252,    0.10001,    0.10022,   57.73748,  128.8433 ,\n",
       "          1.64824,    0.10003,   24.33374,    0.1    ,   81.25351,\n",
       "        123.37452,    0.10002,    0.10001,   27.00126,  216.07551,\n",
       "         56.73575,    0.10005,   46.0896 ,   39.75611,  167.07269,\n",
       "          4.48632,   26.00009,    0.10198,    0.10042,    0.10001,\n",
       "         46.71842,    0.10016,    0.10015,   18.08789,    0.1    ,\n",
       "          0.10003,    6.16773,  162.73179,    0.10002,   20.5954 ,\n",
       "          2.10168,    2.348  ,    0.10004,   60.55956,    0.10007,\n",
       "        117.02185,    0.10001,    0.10001,    2.59983,    0.10009,\n",
       "         23.46512,   29.55319,    0.17424,   25.06385,    0.10004,\n",
       "          0.10012,    0.10004,    0.10001,  126.5384 ,    0.10001,\n",
       "         59.16899,    2.21107,    0.10004,  375.49051,  166.05204,\n",
       "        266.32991,  378.24515,   71.05234,   47.95631,  190.20719,\n",
       "          0.11587,    0.16775,    0.10021,    0.10002,    0.10002,\n",
       "         28.70915,   12.0324 ,    2.7092 ,   75.44206,   55.14529,\n",
       "          0.10167,    0.10006,    0.10002,    0.10067,  523.02402,\n",
       "         30.71848,   68.13953,   64.60711,  342.86488,    0.10002,\n",
       "          0.1    ,    0.10002,   42.23137,  120.15262,    0.1118 ,\n",
       "          0.10003,    0.10003,    0.10004,   10.84458,    0.10474,\n",
       "          0.10023,    0.10003,    0.10079,    0.10002,    0.1    ,\n",
       "          0.10046,    0.10001,    0.10033,    0.1    ,   26.55422,\n",
       "          0.12057,   20.64026,   17.21806,   83.87896,    0.1    ,\n",
       "         30.14234,   71.69265,    0.10096,    0.21688,   19.68627,\n",
       "         67.2631 ,    0.10003,   61.25517,    3.60781,    1.50904,\n",
       "          0.10004,    0.10003,  127.94469,  103.08603,    0.10002,\n",
       "        101.48786,    0.10007,    9.1044 ,    0.10002,    0.10001,\n",
       "         50.75522,    0.10004,    0.10005,    0.10004,    0.10003,\n",
       "          0.10223,    0.10687,    1.62703,    0.1003 ,    0.10604,\n",
       "          0.10003,   66.40887,    3.21874,    0.10003,   22.04578,\n",
       "        155.63303,    2.29846,   73.27174,    0.1    ,    0.10006,\n",
       "          0.10012,   11.0727 ,  113.67686,   28.30907,  161.04849,\n",
       "         32.47919,    0.10002,   72.04281,  248.47217,  216.42693,\n",
       "         36.73508,    0.10135,   12.27982,    0.14244,    7.50541,\n",
       "         16.12217,    0.10001,    0.10001,    0.10001,    0.10001,\n",
       "          0.1    ,    0.10002,  472.58418,  197.643  ,    0.10002,\n",
       "          3.4505 ,    0.10003,    0.10002,   34.99964,    0.5249 ,\n",
       "        267.88091,   27.53901,    0.11377,   39.47479,    0.1    ,\n",
       "        167.7717 ,    0.10001,    0.10006,   14.53427,    0.10003,\n",
       "        130.49913,   52.25483,  106.54259,    0.10003,    0.56118,\n",
       "          0.10007,    5.53188,    0.10005,   37.23524,    0.10004,\n",
       "          0.10002,    0.10002,   36.91767,    7.80446,    0.10004,\n",
       "          0.10001,    0.10002,    0.10002,   31.09118,   53.735  ,\n",
       "         13.10931,    0.10003,    0.10014,    0.10009,   16.43353,\n",
       "         29.53758,    0.10011,    0.60767,   38.85378,    0.10006,\n",
       "          3.75629,    0.10002,    0.10005,    0.10005,   57.90992,\n",
       "         55.05484,    9.47928,    0.10002,    2.80514,   12.55172,\n",
       "        297.40551,    0.10021,   13.32049,   30.5975 ,    0.10002,\n",
       "          1.58907,   22.63185,    0.10002,   12.75794,    0.12139,\n",
       "          2.34496,    0.10013,    2.0403 ,    0.10001,    0.10114,\n",
       "         71.76088,    0.10002,    0.10004,   11.00107,    9.57248,\n",
       "         12.09226,    5.95462,    4.09556,    0.1    ,  192.83953,\n",
       "        701.85827,  240.67487,    0.10003,    0.10002,    6.73834,\n",
       "          2.05502,    1.01076,    0.10271,    0.10006,   49.88865,\n",
       "         10.20497,    0.10002,   32.84855,    2.67536,  231.99271,\n",
       "          0.67529,   26.05931,  354.28557,   64.07424,   28.15784,\n",
       "          0.10033,   43.90718,    0.10005,   54.79742,    0.10017,\n",
       "        178.29716,  143.7834 ,    0.10002,    4.52275,    0.11848,\n",
       "          0.10003,    0.10923,    4.31459,    0.1    ,    0.1    ,\n",
       "          6.16935,  157.84562,    8.45932,   44.43905,    0.10003,\n",
       "          9.72021,    2.3288 ,    0.48822,    0.10003,    0.1    ,\n",
       "         44.43911,   61.82381,    4.96276,   32.25967,    0.23515,\n",
       "          0.10004,    0.10003,    0.10001,    0.10002,    0.11414,\n",
       "         12.80475,   21.47911,    0.10001,    0.10003,    0.12218,\n",
       "        109.5693 ,    0.10013,    0.1    ,    0.10004,    0.1001 ,\n",
       "          0.10009,    5.59736,  201.75986,    0.10021,   19.72608,\n",
       "         15.90317,    0.10076,   39.70824,    0.10006,   47.31673,\n",
       "         94.38845,    0.10002,  106.80425,    0.10098,    0.46369,\n",
       "        149.01525,    0.10005,   92.5452 ,  112.24981,   31.06889,\n",
       "          0.1    ,   56.87803,  331.96376,    0.10001,    0.10001,\n",
       "         14.46338,    0.1    ,    0.10004,  243.9343 ,    0.10004,\n",
       "          0.10773,   29.05197,    2.66056,   83.76779,    0.19519,\n",
       "         70.6459 ,   39.02524,   25.26908,    0.10079,    0.34936,\n",
       "          0.10002,    2.55692,    0.10011,    0.1    ,    0.1    ,\n",
       "       1204.97256,  342.56047,  134.42204,  120.30506,  102.4441 ])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_word_distributions = np.array([row / row.sum() for row in lda.components_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that each topic's word distribution sums to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_word_distributions.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also print out what the probabilities for the different words are for a specific topic. This isn't very easy to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00349 0.      0.      0.      0.      0.      0.      0.00305 0.00631\n",
      " 0.      0.00113 0.      0.00012 0.00001 0.      0.00176 0.      0.\n",
      " 0.      0.      0.00197 0.00141 0.00318 0.00074 0.      0.00227 0.\n",
      " 0.0031  0.      0.      0.00002 0.      0.      0.00021 0.001   0.\n",
      " 0.      0.      0.      0.      0.      0.00285 0.00329 0.      0.\n",
      " 0.      0.      0.      0.0004  0.      0.      0.      0.      0.\n",
      " 0.      0.00166 0.      0.      0.      0.      0.00041 0.      0.\n",
      " 0.00617 0.00381 0.      0.00245 0.      0.      0.00103 0.00112 0.\n",
      " 0.      0.00041 0.00124 0.      0.00207 0.00131 0.00144 0.0005  0.\n",
      " 0.      0.      0.00059 0.      0.0002  0.      0.      0.      0.00084\n",
      " 0.0021  0.00107 0.      0.      0.00008 0.00024 0.00128 0.00211 0.\n",
      " 0.      0.      0.      0.      0.      0.00116 0.      0.0036  0.00287\n",
      " 0.      0.      0.      0.      0.      0.      0.00038 0.00243 0.\n",
      " 0.      0.      0.      0.      0.      0.      0.      0.00093 0.\n",
      " 0.00089 0.00019 0.0003  0.      0.      0.      0.      0.      0.\n",
      " 0.      0.      0.00015 0.      0.00354 0.      0.      0.      0.\n",
      " 0.      0.      0.      0.00001 0.00628 0.      0.00171 0.      0.\n",
      " 0.00267 0.00255 0.00093 0.00008 0.00037 0.      0.      0.00061 0.00941\n",
      " 0.00544 0.      0.      0.      0.00284 0.      0.00079 0.      0.\n",
      " 0.00299 0.      0.      0.00029 0.0002  0.00107 0.00019 0.00084 0.00125\n",
      " 0.00056 0.00065 0.      0.00036 0.      0.      0.      0.      0.\n",
      " 0.00084 0.00222 0.      0.00153 0.00076 0.0017  0.0004  0.      0.0021\n",
      " 0.00487 0.      0.      0.00071 0.00101 0.00032 0.      0.00084 0.00018\n",
      " 0.      0.      0.00238 0.00003 0.      0.00213 0.00042 0.00088 0.00023\n",
      " 0.00195 0.      0.      0.      0.      0.      0.00081 0.      0.\n",
      " 0.      0.      0.      0.      0.00468 0.      0.      0.      0.00028\n",
      " 0.      0.00118 0.      0.      0.      0.      0.      0.00199 0.0009\n",
      " 0.00139 0.      0.00112 0.      0.      0.      0.      0.      0.\n",
      " 0.00074 0.00009 0.      0.00519 0.00243 0.00007 0.00193 0.00078 0.00148\n",
      " 0.      0.      0.00084 0.00124 0.      0.00079 0.00001 0.00075 0.\n",
      " 0.00033 0.      0.00108 0.      0.00492 0.00008 0.      0.00001 0.00085\n",
      " 0.      0.00059 0.00101 0.00051 0.      0.      0.0011  0.00402 0.\n",
      " 0.00418 0.00151 0.      0.00098 0.00021 0.00239 0.      0.      0.\n",
      " 0.00398 0.      0.0001  0.00015 0.      0.00008 0.      0.      0.\n",
      " 0.      0.      0.00361 0.00238 0.00077 0.00022 0.00004 0.      0.\n",
      " 0.      0.      0.      0.      0.      0.00441 0.00464 0.0003  0.001\n",
      " 0.00039 0.00472 0.      0.      0.      0.00047 0.      0.      0.\n",
      " 0.      0.00001 0.0029  0.      0.      0.00136 0.      0.00174 0.0034\n",
      " 0.00061 0.      0.      0.      0.00446 0.      0.      0.00035 0.\n",
      " 0.      0.      0.      0.00019 0.00034 0.      0.      0.00017 0.\n",
      " 0.00505 0.      0.      0.      0.00159 0.00008 0.      0.      0.00172\n",
      " 0.00064 0.00004 0.      0.      0.      0.      0.00279 0.      0.\n",
      " 0.      0.      0.      0.      0.      0.00156 0.      0.00057 0.\n",
      " 0.      0.00348 0.      0.00034 0.00005 0.      0.00012 0.00018 0.00174\n",
      " 0.      0.00172 0.      0.      0.00243 0.      0.0017  0.      0.\n",
      " 0.      0.00167 0.      0.      0.01418 0.      0.      0.00023 0.\n",
      " 0.00209 0.      0.      0.0013  0.      0.      0.      0.      0.0061\n",
      " 0.      0.00201 0.00431 0.01683 0.00761 0.      0.      0.      0.00723\n",
      " 0.      0.      0.00013 0.      0.00087 0.01618 0.00154 0.0023  0.00086\n",
      " 0.00011 0.00021 0.      0.00021 0.00142 0.      0.00075 0.00013 0.00112\n",
      " 0.00022 0.00029 0.      0.      0.      0.00035 0.      0.00312 0.00122\n",
      " 0.      0.00547 0.00997 0.      0.00466 0.00157 0.      0.0016  0.\n",
      " 0.      0.      0.      0.00008 0.00001 0.00133 0.      0.      0.\n",
      " 0.00019 0.00135 0.00261 0.001   0.0011  0.00028 0.      0.00016 0.\n",
      " 0.00055 0.      0.00084 0.      0.      0.00247 0.0004  0.      0.\n",
      " 0.      0.      0.      0.00068 0.      0.      0.00165 0.      0.\n",
      " 0.      0.      0.00165 0.00113 0.00199 0.00115 0.      0.00389 0.\n",
      " 0.      0.      0.      0.      0.00071 0.      0.00268 0.00009 0.00001\n",
      " 0.00043 0.00225 0.00015 0.      0.00059 0.00174 0.      0.      0.\n",
      " 0.01049 0.      0.00064 0.00429 0.      0.00315 0.00029 0.00112 0.\n",
      " 0.      0.0001  0.00599 0.00197 0.00052 0.00359 0.00053 0.00366 0.00172\n",
      " 0.00084 0.      0.0002  0.0075  0.00153 0.00139 0.0009  0.00159 0.0011\n",
      " 0.00245 0.00104 0.      0.00431 0.00372 0.00094 0.      0.00058 0.00121\n",
      " 0.      0.00001 0.      0.00037 0.      0.00166 0.00658 0.00003 0.00031\n",
      " 0.00235 0.00152 0.      0.00433 0.00008 0.00028 0.      0.      0.\n",
      " 0.      0.00105 0.      0.      0.      0.      0.0004  0.      0.\n",
      " 0.      0.      0.00054 0.00001 0.      0.00492 0.00258 0.      0.00164\n",
      " 0.00009 0.      0.      0.      0.      0.00392 0.      0.0013  0.00167\n",
      " 0.      0.      0.      0.      0.01505 0.      0.      0.      0.00178\n",
      " 0.00001 0.00181 0.      0.00108 0.00033 0.00142 0.00271 0.      0.0156\n",
      " 0.00188 0.      0.      0.00158 0.00353 0.00005 0.      0.00067 0.\n",
      " 0.00222 0.00338 0.      0.      0.00074 0.00591 0.00155 0.      0.00126\n",
      " 0.00109 0.00457 0.00012 0.00071 0.      0.      0.      0.00128 0.\n",
      " 0.      0.00049 0.      0.      0.00017 0.00445 0.      0.00056 0.00006\n",
      " 0.00006 0.      0.00166 0.      0.0032  0.      0.      0.00007 0.\n",
      " 0.00064 0.00081 0.      0.00069 0.      0.      0.      0.      0.00346\n",
      " 0.      0.00162 0.00006 0.      0.01027 0.00454 0.00729 0.01035 0.00194\n",
      " 0.00131 0.0052  0.      0.      0.      0.      0.      0.00079 0.00033\n",
      " 0.00007 0.00206 0.00151 0.      0.      0.      0.      0.01431 0.00084\n",
      " 0.00186 0.00177 0.00938 0.      0.      0.      0.00116 0.00329 0.\n",
      " 0.      0.      0.      0.0003  0.      0.      0.      0.      0.\n",
      " 0.      0.      0.      0.      0.      0.00073 0.      0.00056 0.00047\n",
      " 0.0023  0.      0.00082 0.00196 0.      0.00001 0.00054 0.00184 0.\n",
      " 0.00168 0.0001  0.00004 0.      0.      0.0035  0.00282 0.      0.00278\n",
      " 0.      0.00025 0.      0.      0.00139 0.      0.      0.      0.\n",
      " 0.      0.      0.00004 0.      0.      0.      0.00182 0.00009 0.\n",
      " 0.0006  0.00426 0.00006 0.002   0.      0.      0.      0.0003  0.00311\n",
      " 0.00077 0.00441 0.00089 0.      0.00197 0.0068  0.00592 0.00101 0.\n",
      " 0.00034 0.      0.00021 0.00044 0.      0.      0.      0.      0.\n",
      " 0.      0.01293 0.00541 0.      0.00009 0.      0.      0.00096 0.00001\n",
      " 0.00733 0.00075 0.      0.00108 0.      0.00459 0.      0.      0.0004\n",
      " 0.      0.00357 0.00143 0.00292 0.      0.00002 0.      0.00015 0.\n",
      " 0.00102 0.      0.      0.      0.00101 0.00021 0.      0.      0.\n",
      " 0.      0.00085 0.00147 0.00036 0.      0.      0.      0.00045 0.00081\n",
      " 0.      0.00002 0.00106 0.      0.0001  0.      0.      0.      0.00158\n",
      " 0.00151 0.00026 0.      0.00008 0.00034 0.00814 0.      0.00036 0.00084\n",
      " 0.      0.00004 0.00062 0.      0.00035 0.      0.00006 0.      0.00006\n",
      " 0.      0.      0.00196 0.      0.      0.0003  0.00026 0.00033 0.00016\n",
      " 0.00011 0.      0.00528 0.0192  0.00659 0.      0.      0.00018 0.00006\n",
      " 0.00003 0.      0.      0.00137 0.00028 0.      0.0009  0.00007 0.00635\n",
      " 0.00002 0.00071 0.00969 0.00175 0.00077 0.      0.0012  0.      0.0015\n",
      " 0.      0.00488 0.00393 0.      0.00012 0.      0.      0.      0.00012\n",
      " 0.      0.      0.00017 0.00432 0.00023 0.00122 0.      0.00027 0.00006\n",
      " 0.00001 0.      0.      0.00122 0.00169 0.00014 0.00088 0.00001 0.\n",
      " 0.      0.      0.      0.      0.00035 0.00059 0.      0.      0.\n",
      " 0.003   0.      0.      0.      0.      0.      0.00015 0.00552 0.\n",
      " 0.00054 0.00044 0.      0.00109 0.      0.00129 0.00258 0.      0.00292\n",
      " 0.      0.00001 0.00408 0.      0.00253 0.00307 0.00085 0.      0.00156\n",
      " 0.00908 0.      0.      0.0004  0.      0.      0.00667 0.      0.\n",
      " 0.00079 0.00007 0.00229 0.00001 0.00193 0.00107 0.00069 0.      0.00001\n",
      " 0.      0.00007 0.      0.      0.      0.03297 0.00937 0.00368 0.00329\n",
      " 0.0028 ]\n"
     ]
    }
   ],
   "source": [
    "print(topic_word_distributions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, usually people do something like looking at the most probable words per topic, and try to use these words to interpret what the different topics correspond to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying the top 20 words per topic and their probabilities within the topic...\n",
      "\n",
      "[Topic 0]\n",
      "year : 0.03297118094738992\n",
      "team : 0.019204666343880843\n",
      "good : 0.016830138855415535\n",
      "gun : 0.016178567835481206\n",
      "new : 0.015603672962549789\n",
      "mr : 0.01505081138184353\n",
      "president : 0.01431129643505745\n",
      "games : 0.014180220098801988\n",
      "season : 0.012931131385430669\n",
      "league : 0.010491378550696718\n",
      "players : 0.010349770248161931\n",
      "play : 0.010274396314227859\n",
      "hockey : 0.009974003061194872\n",
      "time : 0.009694174123867904\n",
      "best : 0.009409285751708664\n",
      "price : 0.009381674293086522\n",
      "years : 0.009373344778562082\n",
      "win : 0.009083391252969808\n",
      "stephanopoulos : 0.008137787607873974\n",
      "got : 0.007609708926778402\n",
      "\n",
      "[Topic 1]\n",
      "edu : 0.04029110605206494\n",
      "file : 0.03401605842663014\n",
      "com : 0.0222023046148782\n",
      "ftp : 0.014891870249451676\n",
      "available : 0.014872344410171648\n",
      "program : 0.013900099687007832\n",
      "files : 0.01346941290100786\n",
      "mail : 0.012731186252528216\n",
      "list : 0.012302522449955312\n",
      "server : 0.012191608776669492\n",
      "pub : 0.011781900926012206\n",
      "send : 0.011618081633255326\n",
      "information : 0.011328191056635236\n",
      "email : 0.01081941950112364\n",
      "faq : 0.010338836510401675\n",
      "use : 0.010135228627400551\n",
      "anonymous : 0.0096746685473591\n",
      "entry : 0.009404082662349716\n",
      "source : 0.008823763832099402\n",
      "sun : 0.00874015157493989\n",
      "\n",
      "[Topic 2]\n",
      "space : 0.017520829980870203\n",
      "government : 0.014764857355699977\n",
      "law : 0.013984172594173403\n",
      "public : 0.012742615885066314\n",
      "new : 0.011835560307473868\n",
      "university : 0.011455663004367143\n",
      "use : 0.010558752454925957\n",
      "information : 0.010523671907643292\n",
      "national : 0.010005648977110933\n",
      "research : 0.009837721562709864\n",
      "state : 0.009253714677482205\n",
      "states : 0.008693409057582443\n",
      "data : 0.00813717432781462\n",
      "general : 0.007869014386848885\n",
      "1993 : 0.007725106577330434\n",
      "privacy : 0.007621978472840703\n",
      "nasa : 0.007618877183926805\n",
      "control : 0.007514320482818127\n",
      "center : 0.007103239931985117\n",
      "technology : 0.006967116433727473\n",
      "\n",
      "[Topic 3]\n",
      "people : 0.025895408882525735\n",
      "don : 0.019051730185797967\n",
      "just : 0.016890775051117413\n",
      "think : 0.014747845951330904\n",
      "know : 0.014199565695037608\n",
      "like : 0.014113213308107714\n",
      "said : 0.010874414405776101\n",
      "time : 0.010682271372211054\n",
      "right : 0.010536202795306688\n",
      "did : 0.009462650925254166\n",
      "say : 0.008821296230055444\n",
      "going : 0.007917870731667824\n",
      "want : 0.007522594449012154\n",
      "ve : 0.007291645923099386\n",
      "way : 0.007098601161894282\n",
      "didn : 0.007067349290356842\n",
      "make : 0.006286341816011715\n",
      "really : 0.006220033365741903\n",
      "years : 0.006030323009314076\n",
      "ll : 0.005758539222688519\n",
      "\n",
      "[Topic 4]\n",
      "10 : 0.04722606222503568\n",
      "00 : 0.03326765379925959\n",
      "11 : 0.031173500801189198\n",
      "12 : 0.030123700377868004\n",
      "15 : 0.02990359767341554\n",
      "25 : 0.029398719591354996\n",
      "20 : 0.028711218264319927\n",
      "14 : 0.026568760431583533\n",
      "16 : 0.024643647803909676\n",
      "17 : 0.023216760421633384\n",
      "13 : 0.02289261427726245\n",
      "18 : 0.020309457032141442\n",
      "24 : 0.01874689898563933\n",
      "40 : 0.01741601274278343\n",
      "30 : 0.017264702358222795\n",
      "55 : 0.017249979067923173\n",
      "19 : 0.016906732935191186\n",
      "21 : 0.01649286991993608\n",
      "23 : 0.015481325326843542\n",
      "22 : 0.015325721360768468\n",
      "\n",
      "[Topic 5]\n",
      "windows : 0.027479651671251874\n",
      "db : 0.020081302606172316\n",
      "software : 0.019461723980915148\n",
      "dos : 0.01664587139692074\n",
      "card : 0.01662946225244954\n",
      "image : 0.014905777621753494\n",
      "disk : 0.014882547183197302\n",
      "graphics : 0.014394671626531581\n",
      "data : 0.014324999278027262\n",
      "pc : 0.012852843991338195\n",
      "color : 0.012760377141738092\n",
      "mac : 0.012521671837075712\n",
      "memory : 0.012288066764058235\n",
      "window : 0.011725439246988113\n",
      "version : 0.011359671702651977\n",
      "use : 0.011144096720246771\n",
      "display : 0.01070306970233045\n",
      "using : 0.010212944450226597\n",
      "bit : 0.010158024468027209\n",
      "screen : 0.009825308617938965\n",
      "\n",
      "[Topic 6]\n",
      "key : 0.036657227264248915\n",
      "thanks : 0.033309192242443476\n",
      "know : 0.027546143029699155\n",
      "does : 0.023024362368053008\n",
      "chip : 0.020030336089088198\n",
      "use : 0.01733602330954036\n",
      "encryption : 0.017136837158848565\n",
      "help : 0.016496899538802737\n",
      "like : 0.016002653922314588\n",
      "mail : 0.015804000270816323\n",
      "need : 0.015344271069585632\n",
      "keys : 0.01374776217804683\n",
      "looking : 0.013312665254122537\n",
      "clipper : 0.012823434108060605\n",
      "used : 0.012362006854300478\n",
      "sound : 0.012103168531042697\n",
      "hi : 0.011949674369115254\n",
      "advance : 0.010790313074703371\n",
      "information : 0.010637453739816572\n",
      "bit : 0.0102371342314524\n",
      "\n",
      "[Topic 7]\n",
      "god : 0.036778972973209936\n",
      "jesus : 0.017553829079132426\n",
      "does : 0.015816424138174422\n",
      "believe : 0.013958023130356151\n",
      "game : 0.01217174589515135\n",
      "people : 0.011042810079440597\n",
      "say : 0.011006544304419672\n",
      "christian : 0.010853081080413905\n",
      "true : 0.01063009302542626\n",
      "bible : 0.01033931356085632\n",
      "think : 0.009774171445882909\n",
      "church : 0.009668704701033187\n",
      "life : 0.008833494899372603\n",
      "way : 0.00785269950643299\n",
      "religion : 0.007590975235037148\n",
      "christians : 0.0075449880919840175\n",
      "christ : 0.007539616094334565\n",
      "faith : 0.007439660543789094\n",
      "point : 0.007427660316865333\n",
      "good : 0.0071864568563567025\n",
      "\n",
      "[Topic 8]\n",
      "drive : 0.02229573986430297\n",
      "power : 0.019507341681994316\n",
      "like : 0.018522559790376477\n",
      "just : 0.016924517655010525\n",
      "car : 0.016675980375469995\n",
      "use : 0.01546275692548463\n",
      "scsi : 0.013950686273385776\n",
      "ve : 0.013925867683506213\n",
      "good : 0.011218067098463956\n",
      "speed : 0.011115387347522961\n",
      "hard : 0.010994071990946796\n",
      "used : 0.010715407919410558\n",
      "don : 0.01021285486454343\n",
      "problem : 0.010109223796813019\n",
      "work : 0.009567260783500339\n",
      "drives : 0.00822283821010156\n",
      "buy : 0.008009702543935435\n",
      "better : 0.007788251352635239\n",
      "high : 0.00775831079830328\n",
      "does : 0.0073133797599765175\n",
      "\n",
      "[Topic 9]\n",
      "ax : 0.7750508616859831\n",
      "max : 0.05676346028145709\n",
      "g9v : 0.017637993832254468\n",
      "b8f : 0.015315278818004917\n",
      "a86 : 0.012646093392057575\n",
      "145 : 0.010168944298400002\n",
      "pl : 0.01012365428397651\n",
      "1d9 : 0.008174180919536261\n",
      "1t : 0.006531951660828925\n",
      "0t : 0.006459809882532865\n",
      "bhj : 0.006110218348574932\n",
      "giz : 0.005453049410747112\n",
      "3t : 0.005447285579836965\n",
      "34u : 0.005285937363655366\n",
      "2di : 0.005090173874145282\n",
      "75u : 0.004632619379523059\n",
      "wm : 0.004518731940833898\n",
      "2tm : 0.004222775559250272\n",
      "7ey : 0.0036648191849338423\n",
      "bxn : 0.0032500074436691783\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_top_words = 20\n",
    "\n",
    "print('Displaying the top %d words per topic and their probabilities within the topic...' % num_top_words)\n",
    "print()\n",
    "\n",
    "for topic_idx in range(num_topics):\n",
    "    print('[Topic ', topic_idx, ']', sep='')\n",
    "    sort_indices = np.argsort(-topic_word_distributions[topic_idx])\n",
    "    for rank in range(num_top_words):\n",
    "        word_idx = sort_indices[rank]\n",
    "        print(tf_vectorizer.get_feature_names()[word_idx], ':', topic_word_distributions[topic_idx, word_idx])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `transform()` function to figure out for each document, what fraction of it is explained by each of the topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_matrix = lda.transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topic_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00182, 0.00182, 0.00182, 0.00182, 0.00182, 0.00182, 0.00182,\n",
       "       0.82791, 0.15754, 0.00182])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topic_matrix[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing co-occurrences of words\n",
    "\n",
    "Here, we count the number of newsgroup posts in which two words both occur. This part of the demo should feel like a review of co-occurrence analysis from earlier in the course, except now we use scikit-learn's built-in CountVectorizer. Conceptually everything else in the same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "word1 = 'year'\n",
    "word2 = 'team'\n",
    "\n",
    "word1_column_idx = tf_vectorizer.vocabulary_[word1]\n",
    "word2_column_idx = tf_vectorizer.vocabulary_[word2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tf.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       ...,\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf[:, word1_column_idx].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_with_word1 = (tf[:, word1_column_idx].toarray().flatten() > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_with_word2 = (tf[:, word2_column_idx].toarray().flatten() > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_with_both_word1_and_word2 = documents_with_word1 * documents_with_word2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compute the log of the conditional probability of word 1 appearing given that word 2 appeared, where we add in a little bit of a fudge factor in the numerator (in this case, it's actually not needed but some times you do have two words that do not co-occur for which you run into a numerical issue due to taking the log of 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.5482462194376105"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eps = 0.1\n",
    "np.log2((documents_with_both_word1_and_word2.sum() + eps) / documents_with_word2.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_see_word1_given_see_word2(word1, word2, vectorizer, eps=0.1):\n",
    "    word1_column_idx = vectorizer.vocabulary_[word1]\n",
    "    word2_column_idx = vectorizer.vocabulary_[word2]\n",
    "    documents_with_word1 = (tf[:, word1_column_idx].toarray().flatten() > 0)\n",
    "    documents_with_word2 = (tf[:, word2_column_idx].toarray().flatten() > 0)\n",
    "    documents_with_both_word1_and_word2 = documents_with_word1 * documents_with_word2\n",
    "    return np.log2((documents_with_both_word1_and_word2.sum() + eps) / documents_with_word2.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic coherence\n",
    "\n",
    "The below code shows how one implements the topic coherence calculation from lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Topic 0]\n",
      "Coherence: -1356.3836721926853\n",
      "\n",
      "[Topic 1]\n",
      "Coherence: -969.252344768849\n",
      "\n",
      "[Topic 2]\n",
      "Coherence: -1038.5936491181455\n",
      "\n",
      "[Topic 3]\n",
      "Coherence: -752.9744085675202\n",
      "\n",
      "[Topic 4]\n",
      "Coherence: -641.5683154733748\n",
      "\n",
      "[Topic 5]\n",
      "Coherence: -1155.6763255419658\n",
      "\n",
      "[Topic 6]\n",
      "Coherence: -1177.3645847380105\n",
      "\n",
      "[Topic 7]\n",
      "Coherence: -948.0033411181123\n",
      "\n",
      "[Topic 8]\n",
      "Coherence: -1054.2809655411477\n",
      "\n",
      "[Topic 9]\n",
      "Coherence: -217.079440424438\n",
      "\n",
      "Average coherence: -931.1177047484249\n"
     ]
    }
   ],
   "source": [
    "average_coherence = 0\n",
    "for topic_idx in range(num_topics):\n",
    "    print('[Topic ', topic_idx, ']', sep='')\n",
    "    sort_indices = np.argsort(topic_word_distributions[topic_idx])[::-1]\n",
    "    coherence = 0.\n",
    "    for top_word_idx1 in sort_indices[:num_top_words]:\n",
    "        word1 = tf_vectorizer.get_feature_names()[top_word_idx1]\n",
    "        for top_word_idx2 in sort_indices[:num_top_words]:\n",
    "            word2 = tf_vectorizer.get_feature_names()[top_word_idx2]\n",
    "            if top_word_idx1 != top_word_idx2:\n",
    "                coherence += prob_see_word1_given_see_word2(word1, word2, tf_vectorizer, 0.1)\n",
    "    print('Coherence:', coherence)\n",
    "    print()\n",
    "    average_coherence += coherence\n",
    "average_coherence /= num_topics\n",
    "print('Average coherence:', average_coherence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of unique words\n",
    "\n",
    "The below code shows how one implements the number of unique words calculation from lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Topic 0]\n",
      "Number of unique top words: 16\n",
      "\n",
      "[Topic 1]\n",
      "Number of unique top words: 17\n",
      "\n",
      "[Topic 2]\n",
      "Number of unique top words: 16\n",
      "\n",
      "[Topic 3]\n",
      "Number of unique top words: 9\n",
      "\n",
      "[Topic 4]\n",
      "Number of unique top words: 20\n",
      "\n",
      "[Topic 5]\n",
      "Number of unique top words: 17\n",
      "\n",
      "[Topic 6]\n",
      "Number of unique top words: 12\n",
      "\n",
      "[Topic 7]\n",
      "Number of unique top words: 14\n",
      "\n",
      "[Topic 8]\n",
      "Number of unique top words: 12\n",
      "\n",
      "[Topic 9]\n",
      "Number of unique top words: 20\n",
      "\n",
      "Average number of unique top words: 15.3\n"
     ]
    }
   ],
   "source": [
    "average_number_of_unique_top_words = 0\n",
    "for topic_idx1 in range(num_topics):\n",
    "    print('[Topic ', topic_idx1, ']', sep='')\n",
    "    sort_indices1 = np.argsort(topic_word_distributions[topic_idx1])[::-1]\n",
    "    num_unique_top_words = 0\n",
    "    for top_word_idx1 in sort_indices1[:num_top_words]:\n",
    "        word1 = tf_vectorizer.get_feature_names()[top_word_idx1]\n",
    "        break_ = False\n",
    "        for topic_idx2 in range(num_topics):\n",
    "            if topic_idx1 != topic_idx2:\n",
    "                sort_indices2 = np.argsort(topic_word_distributions[topic_idx2])[::-1]\n",
    "                for top_word_idx2 in sort_indices2[:num_top_words]:\n",
    "                    word2 = tf_vectorizer.get_feature_names()[top_word_idx2]\n",
    "                    if word1 == word2:\n",
    "                        break_ = True\n",
    "                        break\n",
    "                if break_:\n",
    "                    break\n",
    "        else:\n",
    "            num_unique_top_words += 1\n",
    "    print('Number of unique top words:', num_unique_top_words)\n",
    "    print()\n",
    "    \n",
    "    average_number_of_unique_top_words += num_unique_top_words\n",
    "average_number_of_unique_top_words /= num_topics\n",
    "print('Average number of unique top words:', average_number_of_unique_top_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
